#!/usr/bin/env python3
from __future__ import annotations

import argparse
import os
import re
import subprocess
import sys
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
from typing import Iterable

REPO_ROOT = Path(__file__).resolve().parents[1]
DEFAULT_OUTPUT = REPO_ROOT / "docs" / "reports" / "code_audit.md"

CODE_EXTENSIONS = {
    ".py",
    ".js",
    ".ts",
    ".tsx",
    ".css",
    ".html",
    ".md",
    ".json",
    ".yaml",
    ".yml",
    ".toml",
    ".rs",
    ".c",
    ".h",
    ".cc",
    ".cpp",
    ".txt",
}
SKIP_DIRECTORIES = {
    ".namel3ss",
    ".git",
    ".mypy_cache",
    ".pytest_cache",
    ".ruff_cache",
    ".venv",
    "__pycache__",
    "node_modules",
}

COMPILER_MODULES = {
    "ast",
    "cir",
    "compilation",
    "compiler",
    "contract",
    "format",
    "ir",
    "lang",
    "lexer",
    "lint",
    "module_loader",
    "parser",
    "schema",
    "spec_check",
    "specification",
    "typecheck",
}
UI_MODULES = {
    "editor",
    "icons",
    "studio",
    "templates",
    "ui",
}

DESCRIPTION_OVERRIDES = {
    "cli": "Command-line entrypoints and command routing for developer workflows.",
    "compiler": "Compilation front-end that validates declarations and builds program IR.",
    "parser": "Grammar-aware parsing layer for `.ai` modules and declarations.",
    "lexer": "Tokenization and scan payload generation for source files.",
    "module_loader": "Project/module resolution and source loading pipeline.",
    "ir": "Intermediate representation models, lowering passes, and serializers.",
    "runtime": "Deterministic runtime execution engine, providers, and persistence adapters.",
    "ui": "Runtime UI manifest shaping and UI contract enforcement.",
    "studio": "Studio APIs and web assets for inspecting and operating applications.",
    "tests": "Root-level test gate files that validate repository contracts.",
    "core_files": "Top-level package modules that expose shared entrypoints and metadata.",
}

IMPORT_FROM_RE = re.compile(r"^\s*from\s+namel3ss\.([a-zA-Z0-9_]+)")
IMPORT_RE = re.compile(r"^\s*import\s+namel3ss\.([a-zA-Z0-9_]+)")


class AuditFailure(RuntimeError):
    """Raised when the audit cannot be completed deterministically."""


@dataclass(frozen=True)
class ModuleEntry:
    name: str
    path: str
    description: str
    layer: str
    lines_of_code: int
    dependencies: tuple[str, ...]

    def contract_payload(self) -> dict[str, object]:
        return {
            "name": self.name,
            "path": self.path,
            "description": self.description,
            "layer": self.layer,
            "lines_of_code": self.lines_of_code,
        }


def collect_module_entries(repo_root: Path) -> list[ModuleEntry]:
    src_root = repo_root / "src" / "namel3ss"
    tests_root = repo_root / "tests"
    docs_root = repo_root / "docs"
    _require_directory(src_root)
    _require_directory(tests_root)
    _require_directory(docs_root)

    entries: list[ModuleEntry] = []
    entries.extend(_collect_source_entries(repo_root, src_root))
    entries.extend(_collect_test_entries(repo_root, tests_root))
    entries.sort(key=lambda item: item.path)
    return entries


def collect_docs_summary(repo_root: Path) -> list[tuple[str, int]]:
    docs_root = repo_root / "docs"
    _require_directory(docs_root)

    counts: dict[str, int] = {}
    for file_path in _iter_files(docs_root):
        if file_path.suffix.lower() != ".md":
            continue
        rel = file_path.relative_to(docs_root)
        if rel.parts and rel.parts[0] == "reports":
            # Generated reports live under docs/reports; excluding them avoids
            # self-referential drift in check mode.
            continue
        key = rel.parts[0] if len(rel.parts) > 1 else "(root)"
        counts[key] = counts.get(key, 0) + 1
    return sorted(counts.items(), key=lambda item: item[0])


def build_audit_report(repo_root: Path) -> str:
    entries = collect_module_entries(repo_root)
    docs_summary = collect_docs_summary(repo_root)
    entry_points = _entry_points(repo_root)

    lines: list[str] = []
    lines.append("# Codebase Audit Report")
    lines.append("")
    lines.append("Generated by `scripts/audit_codebase.py` with deterministic traversal order.")
    lines.append("")
    lines.append("## Scope")
    lines.append("- Source root: `src/namel3ss`")
    lines.append("- Documentation root: `docs`")
    lines.append("- Test root: `tests`")
    lines.append("")
    lines.append("## Module Inventory")
    lines.append(
        "| name | path | description | layer | lines_of_code | dependencies |"
    )
    lines.append("| --- | --- | --- | --- | ---: | --- |")
    for entry in entries:
        deps = ", ".join(entry.dependencies) if entry.dependencies else "none"
        lines.append(
            "| "
            f"{_escape_cell(entry.name)} | "
            f"`{_escape_cell(entry.path)}` | "
            f"{_escape_cell(entry.description)} | "
            f"{entry.layer} | "
            f"{entry.lines_of_code} | "
            f"{_escape_cell(deps)} |"
        )

    lines.append("")
    lines.append("## Data Contract")
    lines.append("Each module row includes the contract fields: `name`, `path`, `description`, `layer`, and `lines_of_code`.")

    lines.append("")
    lines.append("## Documentation Inventory")
    for category, count in docs_summary:
        lines.append(f"- `{category}`: {count} Markdown files")

    lines.append("")
    lines.append("## Key Entry Points")
    for path in entry_points:
        lines.append(f"- `{path}`")

    lines.append("")
    lines.append("## Determinism")
    lines.append("- File scanning is sequential and sorted by relative path.")
    lines.append("- Report rendering has stable section and row order.")
    lines.append("- No timestamps or host-specific paths are emitted.")

    return "\n".join(lines).rstrip() + "\n"


def main(argv: list[str] | None = None) -> int:
    parser = argparse.ArgumentParser(description="Generate deterministic codebase audit report.")
    parser.add_argument("--repo-root", default=str(REPO_ROOT), help="Repository root path.")
    parser.add_argument("--output", default=str(DEFAULT_OUTPUT), help="Markdown report output path.")
    parser.add_argument(
        "--check",
        action="store_true",
        help="Validate that the generated report matches --output without writing changes.",
    )
    args = parser.parse_args(argv)

    repo_root = Path(args.repo_root).resolve()
    output_path = Path(args.output).resolve()

    try:
        report = build_audit_report(repo_root)
    except AuditFailure as err:
        print(f"Audit failed: {err}", file=sys.stderr)
        return 1

    if args.check:
        if not output_path.exists():
            print(f"Audit check failed: missing report file: {output_path}", file=sys.stderr)
            return 1
        existing = output_path.read_text(encoding="utf-8")
        if existing != report:
            print(
                "Audit check failed: generated report differs from committed output. "
                "Re-run scripts/audit_codebase.py and commit the result.",
                file=sys.stderr,
            )
            return 1
        print(f"Audit check passed: {output_path}")
        return 0

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(report, encoding="utf-8")
    print(f"Audit report written: {output_path}")
    return 0


def _collect_source_entries(repo_root: Path, src_root: Path) -> list[ModuleEntry]:
    entries: list[ModuleEntry] = []
    subdirs = sorted(
        path for path in src_root.iterdir() if path.is_dir() and path.name not in SKIP_DIRECTORIES
    )
    for module_dir in subdirs:
        entry = _build_entry_from_directory(repo_root, module_dir, forced_layer=None)
        if entry is not None:
            entries.append(entry)

    root_files = sorted(
        path
        for path in src_root.iterdir()
        if path.is_file() and path.suffix.lower() == ".py"
    )
    if root_files:
        entries.append(
            _build_entry_from_files(
                repo_root=repo_root,
                display_name="core_files",
                display_path="src/namel3ss",
                files=root_files,
                forced_layer="runtime",
            )
        )
    return entries


def _collect_test_entries(repo_root: Path, tests_root: Path) -> list[ModuleEntry]:
    entries: list[ModuleEntry] = []
    subdirs = sorted(
        path for path in tests_root.iterdir() if path.is_dir() and path.name not in SKIP_DIRECTORIES
    )
    for module_dir in subdirs:
        entry = _build_entry_from_directory(repo_root, module_dir, forced_layer="test")
        if entry is not None:
            entries.append(entry)

    root_files = sorted(
        path
        for path in tests_root.iterdir()
        if path.is_file() and path.suffix.lower() == ".py"
    )
    if root_files:
        entries.append(
            _build_entry_from_files(
                repo_root=repo_root,
                display_name="tests",
                display_path="tests",
                files=root_files,
                forced_layer="test",
            )
        )
    return entries


def _build_entry_from_directory(
    repo_root: Path,
    module_dir: Path,
    forced_layer: str | None,
) -> ModuleEntry | None:
    rel_path = module_dir.relative_to(repo_root).as_posix()
    name = module_dir.name
    layer = forced_layer or _classify_layer(module_dir.relative_to(repo_root))
    loc = _count_loc(module_dir)
    if loc == 0:
        # Empty/untracked directories are not part of the deterministic source
        # surface in clean checkouts; skip them to avoid local drift.
        return None
    deps = _collect_dependencies_from_paths(_iter_python_files(module_dir), own_name=name)
    description = _describe_module(name=name, layer=layer)
    return ModuleEntry(
        name=name,
        path=rel_path,
        description=description,
        layer=layer,
        lines_of_code=loc,
        dependencies=deps,
    )


def _build_entry_from_files(
    *,
    repo_root: Path,
    display_name: str,
    display_path: str,
    files: list[Path],
    forced_layer: str,
) -> ModuleEntry:
    layer = forced_layer
    loc = sum(_count_lines(path) for path in files)
    deps = _collect_dependencies_from_paths(files, own_name=display_name)
    description = _describe_module(name=display_name, layer=layer)
    return ModuleEntry(
        name=display_name,
        path=display_path,
        description=description,
        layer=layer,
        lines_of_code=loc,
        dependencies=deps,
    )


def _describe_module(name: str, layer: str) -> str:
    if name in DESCRIPTION_OVERRIDES:
        return DESCRIPTION_OVERRIDES[name]
    normalized = name.replace("_", " ")
    if layer == "compiler":
        return f"Compiler-side module for {normalized} logic and validation."
    if layer == "UI":
        return f"UI-facing module for {normalized} rendering and interaction behavior."
    if layer == "test":
        return f"Automated tests that lock {normalized} behavior and regressions."
    return f"Runtime-oriented module for {normalized} execution and support utilities."


def _classify_layer(relative_path: Path) -> str:
    parts = relative_path.parts
    if not parts:
        return "runtime"
    if parts[0] == "tests":
        return "test"
    if parts[0] != "src" or len(parts) < 3:
        return "runtime"
    module_name = parts[2]
    if module_name in UI_MODULES:
        return "UI"
    if module_name in COMPILER_MODULES:
        return "compiler"
    return "runtime"


def _collect_dependencies_from_paths(paths: Iterable[Path], own_name: str) -> tuple[str, ...]:
    dependencies: set[str] = set()
    for path in paths:
        text = _read_text(path)
        for line in text.splitlines():
            match = IMPORT_FROM_RE.match(line)
            if match:
                dependencies.add(match.group(1))
                continue
            match = IMPORT_RE.match(line)
            if match:
                dependencies.add(match.group(1))
    dependencies.discard(own_name)
    return tuple(sorted(dependencies))


def _iter_python_files(root: Path) -> list[Path]:
    return [path for path in _iter_files(root) if path.suffix.lower() == ".py"]


def _count_loc(root: Path) -> int:
    total = 0
    for file_path in _iter_files(root):
        if file_path.suffix.lower() in CODE_EXTENSIONS:
            total += _count_lines(file_path)
    return total


def _count_lines(path: Path) -> int:
    text = _read_text(path)
    return len(text.splitlines())


def _read_text(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except OSError as err:
        raise AuditFailure(f"Unreadable file: {path} ({err})") from err

def _iter_files(root: Path) -> Iterable[Path]:
    git_root = _find_git_root(root)
    tracked_paths: frozenset[str] | None = None
    if git_root is not None:
        tracked_paths = _tracked_repo_paths(git_root)

    def _on_walk_error(err: OSError) -> None:
        raise AuditFailure(f"Unreadable directory: {err.filename} ({err.strerror})")

    for current, dir_names, file_names in os.walk(root, topdown=True, onerror=_on_walk_error):
        dir_names[:] = sorted(name for name in dir_names if name not in SKIP_DIRECTORIES)
        for file_name in sorted(file_names):
            file_path = Path(current) / file_name
            if file_path.is_file():
                if tracked_paths is not None and git_root is not None:
                    rel_path = file_path.resolve().relative_to(git_root.resolve()).as_posix()
                    if rel_path not in tracked_paths:
                        continue
                yield file_path

def _find_git_root(path: Path) -> Path | None:
    current = path.resolve()
    for candidate in (current, *current.parents):
        if (candidate / ".git").exists():
            return candidate
    return None

@lru_cache(maxsize=8)
def _tracked_repo_paths(git_root: Path) -> frozenset[str] | None:
    try:
        result = subprocess.run(
            ["git", "-C", git_root.as_posix(), "ls-files", "-z"],
            check=True,
            capture_output=True,
        )
    except (OSError, subprocess.CalledProcessError):
        return None

    entries = [part.decode("utf-8") for part in result.stdout.split(b"\0") if part]
    filtered = [
        path
        for path in sorted(entries)
        if not any(segment in SKIP_DIRECTORIES for segment in Path(path).parts)
    ]
    return frozenset(filtered)


def _require_directory(path: Path) -> None:
    if not path.exists():
        raise AuditFailure(f"Missing required path: {path}")
    if not path.is_dir():
        raise AuditFailure(f"Expected directory but found file: {path}")
    try:
        _ = sorted(path.iterdir())
    except OSError as err:
        raise AuditFailure(f"Unreadable directory: {path} ({err})") from err


def _entry_points(repo_root: Path) -> list[str]:
    candidates = [
        "src/namel3ss/__main__.py",
        "src/namel3ss/cli/main.py",
        "src/namel3ss/runtime/run_pipeline.py",
        "src/namel3ss/studio/server.py",
        "tests/contract/test_contract_determinism.py",
    ]
    existing: list[str] = []
    for relative in candidates:
        path = repo_root / relative
        if path.exists():
            existing.append(relative)
    return existing


def _escape_cell(value: str) -> str:
    return value.replace("|", "\\|")


if __name__ == "__main__":
    raise SystemExit(main())
